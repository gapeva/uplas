# uplas-ai-agents/nlp_content_agent/main.py
from fastapi import FastAPI, HTTPException, BackgroundTasks, status
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Optional, Any, Union
import os
import uuid
import time
import httpx # For any potential future internal calls (not primary for this agent)
import logging
import json
import random # For mock generation
import re # Added for regex search in mock

# GCP Clients - Import moved inside __init__ for VertexAILLMClientForNLP if needed
# from google.cloud import aiplatform

SUPPORTED_LANGUAGES = ["en-US", "fr-FR", "es-ES", "de-DE", "pt-BR", "zh-CN", "hi-IN"] 
DEFAULT_LANGUAGE = "en-US" 


# --- Configuration ---
GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID") 
GCP_LOCATION = os.getenv("GCP_LOCATION", "us-central1") 
NLP_LLM_MODEL_NAME = os.getenv("NLP_LLM_MODEL_NAME", "gemini-1.5-flash-001") 

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO) 
logger = logging.getLogger(__name__) 

# --- Pydantic Models for NLP Agent Output ---

class NlpTopic(BaseModel): 
    topic_id: str = Field(default_factory=lambda: f"topic_{uuid.uuid4().hex[:8]}", examples=["topic_superposition_intro"])
    topic_title: str = Field(..., examples=["Understanding Superposition"])
    key_concepts: List[str] = Field(default_factory=list, examples=[
        "Qubits can represent 0, 1, or a combination of both.",
        "Superposition allows quantum computers to perform many calculations at once."
    ])
    content_with_tags: str = Field(..., examples=["A classical bit is either 0 or 1. <analogy type=\"comparison_to_classical_needed\" for_concept=\"bit_vs_qubit\" context_keywords=\"classical_bit,qubit,superposition\" /> ... <difficulty type=\"foundational_info\" />"])
    estimated_complexity_score: Optional[float] = Field(None, ge=0.0, le=1.0, description="Normalized complexity score (0=easiest, 1=hardest)")
    suggested_prerequisites: Optional[List[str]] = Field(default_factory=list, description="List of prerequisite topic names or concept keywords.")


class NlpLesson(BaseModel): 
    lesson_id: str = Field(default_factory=lambda: f"lesson_{uuid.uuid4().hex[:8]}", examples=["lesson_quantum_basics"])
    lesson_title: str = Field(..., examples=["What is a Qubit?"])
    lesson_summary: Optional[str] = Field(None, description="A brief summary of the lesson, generated by LLM.")
    topics: List[NlpTopic] = Field(default_factory=list)


class ProcessedModule(BaseModel): 
    module_id: str = Field(..., examples=["course101_module3_processed"]) 
    source_module_id: str = Field(..., examples=["course101_module3_raw"]) 
    module_title: Optional[str] = Field(None, examples=["Introduction to Quantum Computing"])
    language_code: str = Field(..., examples=["en-US"])
    lessons: List[NlpLesson] = Field(default_factory=list)
    processing_time_ms: Optional[float] = None
    llm_model_used: Optional[str] = None
    module_level_visual_aid_summary: Optional[List[Dict[str,str]]] = Field(default_factory=list, description="Summary of all visual aid suggestions in the module.")

# --- Pydantic Models for API Request ---

class ProcessContentRequest(BaseModel): 
    module_id: str = Field(..., examples=["course101_module3_raw"], description="Unique ID for the raw course module being processed.")
    raw_text_content: str = Field(..., min_length=50, description="The full raw text content of the course module.") 
    language_code: str = Field(DEFAULT_LANGUAGE, examples=SUPPORTED_LANGUAGES)
    module_title: Optional[str] = Field(None, description="Optional title for the module if known.")

    @validator('language_code')
    def validate_language_code(cls, v): 
        if v not in SUPPORTED_LANGUAGES:
            logger.warning(f"NovaSpark Warning: Unsupported language_code '{v}' in ProcessContentRequest. Falling back to default '{DEFAULT_LANGUAGE}'.")
            return DEFAULT_LANGUAGE
        return v

# --- Vertex AI LLM Client Logic (NovaSpark Enhanced for Specificity & Multilingualism) ---
class VertexAILLMClientForNLP: 
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.is_initialized = False 
        if not GCP_PROJECT_ID: 
            logger.error("NovaSpark Critical: GCP_PROJECT_ID not set. Vertex AI LLM client for NLP WILL NOT FUNCTION.")
        else:
            try:
                from google.cloud import aiplatform 
                aiplatform.init(project=GCP_PROJECT_ID, location=GCP_LOCATION)
                self.is_initialized = True
                logger.info(f"NovaSpark: Vertex AI SDK initialized for NLP Agent. Project: {GCP_PROJECT_ID}, Location: {GCP_LOCATION}")
            except Exception as e_init:
                logger.error(f"NovaSpark Critical: Failed to initialize Vertex AI SDK for NLP Agent. Error: {e_init}", exc_info=True)

    async def _call_gemini_api(self, system_prompt: str, user_query: str, is_json_output: bool = True, 
                               pydantic_model_for_schema: Optional[Any] = None) -> str: 
        if not self.is_initialized:
            logger.error("NovaSpark Error: Vertex AI Client for NLP was not initialized. Cannot call Gemini API.")
            if "MOCK_FORCE_SUCCESS" in os.environ and os.environ["MOCK_FORCE_SUCCESS"].lower() == "true": # Allow forcing mock success for testing without init
                 logger.warning("NovaSpark: Proceeding with MOCK response due to MOCK_FORCE_SUCCESS, despite client not initialized.")
            else:
                return json.dumps({"error": "NLP LLM Client not initialized."})

        # Check if we are in actual call mode or mock mode based on an env var for easier testing
        # This allows testing the prompt construction path without making actual API calls if not desired.
        if os.getenv("NLP_AGENT_FORCE_MOCK_API_CALLS", "true").lower() == "true":
            # Mocked responses for NovaSpark framework (Task 5 Enhanced Mock, Task 3 Multilingual)
            if "Your objectives are to structure and enrich this content" in system_prompt and "Divide this lesson snippet into logical topics" in system_prompt:
                logger.info("NovaSpark Mock: Generating ENHANCED mock response for micro_segment_and_enrich_lesson.")
                lang_match = re.search(r"language: \[([a-zA-Z]{2,3}-[a-zA-Z]{2,4})\]", system_prompt)
                mock_lang = lang_match.group(1) if lang_match else DEFAULT_LANGUAGE
                
                topic_titles_map = {
                    "en-US": ["Introduction to Specificity", "Actionable Tag Examples"],
                    "fr-FR": ["Introduction à la Spécificité", "Exemples de Balises Actionnables"],
                }
                key_concepts_map = {
                    "en-US": [["Tag Attributes", "Contextual Keywords", "Actionability"], ["Downstream Agent Use", "Personalization Priming"]],
                    "fr-FR": [["Attributs de Balise", "Mots-clés Contextuels", "Actionnabilité"], ["Utilisation par Agent Aval", "Amorçage de Personnalisation"]],
                }
                content_snippets_map = { # These already contain example specific tags from previous refinement
                    "en-US": [
                        "Specificity in NLP tags is crucial. <analogy type=\"user_profile_driven_analogy_needed\" for_concept=\"tag_specificity\" context_keywords=\"NLP_tags,actionable_data,downstream_processing\" suggested_theme_if_obvious=\"data_pipelines\" /> This allows for better AI Tutor personalization. <visual_aid_suggestion type=\"flowchart\" description=\"Flow from NLP tag generation to AI Tutor using the tag's context_keywords and for_concept attributes to personalize an analogy.\" keywords=\"NLP_agent,AI_Tutor,personalization,tag_attributes,workflow\" complexity=\"moderate\" purpose=\"illustrate_process\" for_text_segment_id=\"nlp_spec_seg_001\" /> This is <difficulty type=\"intermediate_detail\">important</difficulty>.",
                        "For example, an <example domain=\"user_profile_driven_example_needed\" for_concept=\"actionable_visual_aid_tag\" context_keywords=\"visual_aid_suggestion,diagram_generation,specificity\" suggested_domain_if_obvious=\"automated_report_graphics\" /> helps the TTV agent. This section is <difficulty type=\"foundational_info\">fundamental</difficulty>. Any questions? <interactive_question_opportunity text_suggestion=\"How can specific keywords in tags improve AI responses?\" />"
                    ],
                    "fr-FR": [ # Ensuring French mock content for all text fields
                        "La spécificité des balises NLP est cruciale. <analogy type=\"user_profile_driven_analogy_needed\" for_concept=\"specificite_balise\" context_keywords=\"balises_NLP,donnees_actionnables,traitement_aval\" suggested_theme_if_obvious=\"pipelines_de_donnees\" /> Cela permet une meilleure personnalisation par le Tuteur IA. <visual_aid_suggestion type=\"flowchart\" description=\"Flux de la génération de balises NLP au Tuteur IA utilisant les attributs context_keywords et for_concept de la balise pour personnaliser une analogie.\" keywords=\"agent_NLP,Tuteur_IA,personnalisation,attributs_balise,workflow\" complexity=\"moderate\" purpose=\"illustrate_process\" for_text_segment_id=\"nlp_spec_seg_001_fr\" /> C'est <difficulty type=\"intermediate_detail\">important</difficulty>.",
                        "Par exemple, un <example domain=\"user_profile_driven_example_needed\" for_concept=\"balise_aide_visuelle_actionnable\" context_keywords=\"suggestion_aide_visuelle,generation_diagramme,specificite\" suggested_domain_if_obvious=\"graphiques_rapport_automatises\" /> aide l'agent TTV. Cette section est <difficulty type=\"foundational_info\">fondamentale</difficulty>. Des questions? <interactive_question_opportunity text_suggestion=\"Comment des mots-clés spécifiques dans les balises peuvent-ils améliorer les réponses de l'IA?\" />"
                    ],
                }
                
                mock_lesson_detail = {
                  "lesson_title": f"Leçon Enrichie sur la Spécificité des Balises ({mock_lang})" if mock_lang == "fr-FR" else f"Enriched Lesson on Tag Specificity ({mock_lang})",
                  "lesson_summary": f"Cette leçon détaille comment créer des balises d'enrichissement très spécifiques et actionnables pour les agents IA d'Uplas, en {mock_lang}." if mock_lang == "fr-FR" else f"This lesson details how to create highly specific and actionable enrichment tags for Uplas AI agents, in {mock_lang}.",
                  "topics": [
                      {
                          "topic_id": f"nlp_specificity_intro_{uuid.uuid4().hex[:4]}",
                          "topic_title": topic_titles_map.get(mock_lang, topic_titles_map["en-US"])[0],
                          "key_concepts": key_concepts_map.get(mock_lang, key_concepts_map["en-US"])[0],
                          "content_with_tags": content_snippets_map.get(mock_lang, content_snippets_map["en-US"])[0],
                          "estimated_complexity_score": 0.7,
                          "suggested_prerequisites": [f"Balisage NLP de base ({mock_lang})" if mock_lang == "fr-FR" else f"Basic NLP Tagging ({mock_lang})"]
                      },
                      {
                          "topic_id": f"nlp_actionable_examples_{uuid.uuid4().hex[:4]}",
                          "topic_title": topic_titles_map.get(mock_lang, topic_titles_map["en-US"])[1],
                          "key_concepts": key_concepts_map.get(mock_lang, key_concepts_map["en-US"])[1],
                          "content_with_tags": content_snippets_map.get(mock_lang, content_snippets_map["en-US"])[1],
                          "estimated_complexity_score": 0.5
                      }
                  ]
                }
                return json.dumps(mock_lesson_detail)

            elif "segment it into distinct, high-level lessons" in system_prompt: 
                logger.info("NovaSpark Mock: Generating standard mock response for macro_segment_module.")
                lang_match = re.search(r"language: \[([a-zA-Z]{2,3}-[a-zA-Z]{2,4})\]", system_prompt)
                mock_lang = lang_match.group(1) if lang_match else DEFAULT_LANGUAGE
                mock_lessons_data = [
                    {"lesson_title": f"Leçon Simulée 1: Introduction ({mock_lang})" if mock_lang=="fr-FR" else f"Mock Lesson 1: Introduction ({mock_lang})", "text_segment_start_index": 0, "text_segment_end_index": 150},
                    {"lesson_title": f"Leçon Simulée 2: Plongée ({mock_lang})" if mock_lang=="fr-FR" else f"Mock Lesson 2: Deep Dive ({mock_lang})", "text_segment_start_index": 151, "text_segment_end_index": 300}
                ]
                return json.dumps(mock_lessons_data)
            
            logger.warning("NovaSpark Mock: No specific mock response matched in _call_gemini_api for NLP based on system prompt.")
            return json.dumps({"warning": "No specific mock matched", "system_prompt_start": system_prompt[:100]})
        
        # Actual API Call (Mugambi to implement fully)
        from vertexai.generative_models import GenerativeModel, Part, GenerationConfig, HarmCategory, HarmBlockThreshold 

        logger.info(f"NovaSpark: Calling REAL Gemini API for NLP. Model: {self.model_name}. JSON Output: {is_json_output}")
        # logger.debug(f"NovaSpark NLP System Prompt (full): {system_prompt}") # Be careful with logging full prompts in prod
        # logger.debug(f"NovaSpark NLP User Query (full): {user_query}")

        try:
            model = GenerativeModel(self.model_name, system_instruction=[Part.from_text(system_prompt)])
            gen_config_params = {
                "temperature": 0.2, 
                "max_output_tokens": 8192, 
                "top_p": 0.9, "top_k": 35 
            }
            if is_json_output:
                gen_config_params["response_mime_type"] = "application/json"
                if pydantic_model_for_schema:
                    try:
                        gen_config_params["response_schema"] = pydantic_model_for_schema.model_json_schema()
                        logger.info(f"NovaSpark: Using Pydantic schema for {pydantic_model_for_schema.__name__} in Gemini call.")
                    except Exception as e_schema:
                        logger.error(f"NovaSpark Error creating schema for {pydantic_model_for_schema.__name__}: {e_schema}", exc_info=True)
            
            generation_config = GenerationConfig(**gen_config_params)
            safety_settings = {
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            }

            api_response = await model.generate_content_async(
                [Part.from_text(user_query)],
                generation_config=generation_config,
                safety_settings=safety_settings
            )
            
            response_text = ""
            if api_response.candidates:
                candidate = api_response.candidates[0]
                if candidate.finish_reason not in [candidate.FinishReason.STOP, candidate.FinishReason.MAX_TOKENS]:
                    logger.warning(f"NovaSpark LLM Warning: NLP Agent response generation finished with reason: {candidate.finish_reason.name if candidate else 'N/A'}.")
                    # Handle safety or other non-stop reasons by returning an error JSON or specific structure
                    error_payload = {"error": f"Content generation stopped due to: {candidate.finish_reason.name if candidate else 'Unknown Reason'}", "details": str(candidate.safety_ratings if hasattr(candidate, 'safety_ratings') else "No details")}
                    return json.dumps(error_payload)

                if candidate.content and candidate.content.parts:
                    response_text = "".join([part.text for part in candidate.content.parts if part.text])
            
            if not response_text and candidate and candidate.finish_reason == candidate.FinishReason.STOP:
                 response_text = "{}" # Empty JSON if expecting JSON but got nothing
                 logger.warning("NovaSpark LLM Warning: NLP Agent received successful stop but no text content. Assuming empty JSON.")

            # Log token usage if available
            if hasattr(api_response, 'usage_metadata') and api_response.usage_metadata:
                logger.info(f"NovaSpark LLM Token Usage: Prompt={api_response.usage_metadata.prompt_token_count}, Response={api_response.usage_metadata.candidates_token_count}")

            if not response_text: # If still empty after all checks
                logger.error("NovaSpark Error: No content in LLM response or no candidates even after successful stop.")
                return json.dumps({"error": "LLM returned no parsable content."})
            
            logger.info(f"NovaSpark: Received LLM response (length: {len(response_text)} chars).")
            return response_text
        
        except Exception as e_gemini:
            logger.error(f"NovaSpark Error during Gemini API call: {e_gemini}", exc_info=True)
            return json.dumps({"error": f"Gemini API call failed: {str(e_gemini)}"})

    async def macro_segment_module(self, full_text: str, language_code: str, module_title: Optional[str]) -> List[Dict[str, Any]]: 
        system_prompt = ( 
            "You are an expert instructional designer specializing in modular content creation.\n"
            f"**CRITICAL LANGUAGE MANDATE: ALL textual content you generate for the 'lesson_title' field MUST be strictly and exclusively in the specified language: [{language_code}]. Ensure natural phrasing in [{language_code}].**\n\n" # Task 3 Emphasis
            f"Your task is to analyze the provided course module text (which is in language [{language_code}]) "
            f"and segment it into distinct, coherent, high-level lessons. Each lesson should represent a significant unit of learning.\n"
            f"For each identified lesson, provide a concise and engaging 'lesson_title' IN [{language_code}].\n" 
            "Also, provide the exact 'text_segment_start_index' (integer, 0-based) and 'text_segment_end_index' (integer, exclusive) "
            "from the original text that constitutes that lesson.\n"
            f"The overall module is titled: '{module_title if module_title else 'Not Provided'}'.\n"
            "Focus on logical flow, clear topic transitions, and appropriate lesson lengths for online learning.\n"
            "Respond ONLY with a valid JSON list of objects. Each object in the list MUST have three keys: "
            "'lesson_title' (string), 'text_segment_start_index' (integer), and 'text_segment_end_index' (integer)."
        )
        user_query = f"Segment this module text into lessons, ensuring the 'lesson_title' for each is IN LANGUAGE [{language_code}]:\n\n---MODULE TEXT START---\n{full_text}\n---MODULE TEXT END---"

        response_str = "{}"
        try:
            response_str = await self._call_gemini_api(system_prompt, user_query, is_json_output=True)
            lessons_data = json.loads(response_str)
            if not isinstance(lessons_data, list): 
                logger.error(f"NovaSpark Error: LLM response for macro-segmentation was not a list. Response: {response_str[:300]}")
                raise ValueError("LLM response for macro-segmentation was not a list.")
            for item in lessons_data: 
                if not all(k in item for k in ("lesson_title", "text_segment_start_index", "text_segment_end_index")):
                    logger.error(f"NovaSpark Error: Invalid item in macro-segmentation list: {item}. Missing required keys.")
                    raise ValueError("Invalid item structure in LLM response for macro-segmentation.")
                if not (isinstance(item["text_segment_start_index"], int) and isinstance(item["text_segment_end_index"], int) and item["text_segment_start_index"] <= item["text_segment_end_index"]):
                    logger.error(f"NovaSpark Error: Invalid indices in macro-segmentation item: {item}.")
                    raise ValueError("Invalid indices in LLM response for macro-segmentation.")
            return lessons_data
        except json.JSONDecodeError as e: 
            logger.error(f"NovaSpark JSONDecodeError in macro_segment_module: {e}. Response: {response_str[:500]}", exc_info=True)
            raise ValueError(f"Failed to parse LLM response for macro-segmentation: {e}")
        except Exception as e: 
            logger.error(f"NovaSpark Error in macro_segment_module: {e}", exc_info=True)
            raise

    async def micro_segment_and_enrich_lesson(self, lesson_text_snippet: str, lesson_title_from_macro: str, language_code: str) -> Dict[str, Any]: 
        # NovaSpark Enhanced: Task 3 - Added global language mandate. Task 5 refinements already present.
        system_prompt = (
            f"You are an AI pedagogical content specialist for Uplas EdTech. Your task is to meticulously analyze the lesson text snippet provided (lesson title: '{lesson_title_from_macro}', original language of snippet: [{language_code}]) and restructure it into a detailed JSON object.\n"
            f"**CRITICAL LANGUAGE MANDATE: ALL textual content you generate for ANY field in the JSON output (including, but not limited to, 'lesson_summary', all 'topic_title's, all 'key_concepts' strings, and any text within any attribute of any semantic tag such as 'description' in visual_aid_suggestion or 'text_suggestion' in interactive_question_opportunity) MUST be strictly and exclusively in the specified language: [{language_code}]. Do not mix languages within the response. Ensure natural phrasing in [{language_code}].**\n\n"
            "Your objectives are to structure and enrich this content:"
            f"1. 'lesson_summary': Generate a concise summary (1-2 sentences) for this snippet, IN [{language_code}]."
            "2. Divide the snippet into logical, granular 'topics'. For each topic:"
            "   a. 'topic_id': Generate a unique, url-friendly ID (e.g., 't1_concept_x')."
            f"  b. 'topic_title': Create a clear, descriptive title IN [{language_code}]."
            f"  c. 'key_concepts': Extract 2-4 crucial key concepts as a list of short strings, IN [{language_code}]."
            "   d. 'content_with_tags': This is the original text for the topic, but you MUST intelligently intersperse it with XML-like semantic tags where pedagogically appropriate. ALL TEXT generated within tag attributes (like descriptions or suggestions) MUST also be IN [{language_code}]."
            "      Focus on creating HIGHLY SPECIFIC and ACTIONABLE tags:"
            "      - `<analogy type=\"user_profile_driven_analogy_needed\" for_concept=\"[specific_concept_being_explained_by_analogy]\" context_keywords=\"[comma_separated_keywords_from_text_segment_related_to_concept]\" suggested_theme_if_obvious=\"[e.g.,_finance,_sports_if_text_implies_a_theme]\" />` "
            "        (Ensure `for_concept` and `context_keywords` are very specific.)"
            "      - `<example domain=\"user_profile_driven_example_needed\" for_concept=\"[specific_concept_being_illustrated]\" context_keywords=\"[comma_separated_keywords_from_text_segment]\" suggested_domain_if_obvious=\"[e.g.,_software_testing,_medical_diagnosis_if_text_implies]\" />` "
            "        (Ensure `for_concept` and `context_keywords` are specific.)"
            "      - `<interactive_question_opportunity text_suggestion=\"[Suggest_a_brief_checking_question_in_{language_code}_here]\" />` (Question IN [{language_code}].)"
            "      - `<visual_aid_suggestion type=\"[diagram|chart|etc]\" description=\"[Detailed_description_of_visual_IN_{language_code}]\" keywords=\"[keywords_IN_{language_code}]\" complexity=\"[simple|moderate|detailed]\" purpose=\"[purpose]\" for_text_segment_id=\"[lesson_title_slugified]_[topic_index]_[tag_index]\" />` "
            "        (Make `description` and `keywords` specific and actionable. `for_text_segment_id` unique.)"
            "      - `<difficulty type=\"[foundational_info|intermediate_detail|advanced_detail]\" />`"
            f"  e. Optionally: 'estimated_complexity_score' (float, 0.0-1.0) and 'suggested_prerequisites' (list of strings IN [{language_code}])."
            f"FINAL REMINDER: Every piece of text you generate for any field or attribute MUST be IN [{language_code}]. NO EXCEPTIONS."
            "Respond ONLY with a single, valid JSON object. The root object should have 'lesson_title' (use '{lesson_title_from_macro}'), 'lesson_summary', and 'topics' (list of topic objects)."
        )
        user_query = f"Analyze and enrich this lesson text snippet for lesson '{lesson_title_from_macro}', ensuring ALL generated text is IN LANGUAGE [{language_code}]:\n\n---LESSON SNIPPET START---\n{lesson_text_snippet}\n---LESSON SNIPPET END---"

        class MicroSegmentTopic(BaseModel): 
            topic_id: str; topic_title: str; key_concepts: List[str]; content_with_tags: str
            estimated_complexity_score: Optional[float] = None; suggested_prerequisites: Optional[List[str]] = None
        class MicroSegmentRoot(BaseModel):
            lesson_title: str; lesson_summary: str; topics: List[MicroSegmentTopic]
        
        response_str = "{}"
        try:
            response_str = await self._call_gemini_api(system_prompt, user_query, is_json_output=True, pydantic_model_for_schema=MicroSegmentRoot)
            # Basic check for error payload from _call_gemini_api itself before JSON parsing
            if response_str.startswith('{"error":'):
                logger.error(f"NovaSpark Error from _call_gemini_api (possibly during API call or init): {response_str}")
                # Convert to a ValueError that the endpoint can catch and return a 500
                raise ValueError(f"LLM client error: {json.loads(response_str).get('error', 'Unknown LLM client error')}")

            enriched_lesson_data = json.loads(response_str)
            validated_data = MicroSegmentRoot(**enriched_lesson_data) 
            logger.info(f"NovaSpark: Successfully parsed and validated micro-segmentation response for '{lesson_title_from_macro}'.")
            for topic in validated_data.topics:
                # Simple check, more robust validation might be needed depending on LLM consistency
                if not all(getattr(topic, k, None) is not None for k in ("topic_id", "topic_title", "key_concepts", "content_with_tags")):
                     logger.warning(f"NovaSpark Warning: A topic in micro-segmentation might be missing required string/list keys after validation: {getattr(topic, 'topic_id', 'N/A')}")
            return validated_data.model_dump()
            
        except json.JSONDecodeError as e: 
            logger.error(f"NovaSpark JSONDecodeError in micro_segment_and_enrich_lesson: {e}. Response: {response_str[:500]}", exc_info=True)
            raise ValueError(f"Failed to parse LLM response for micro-segmentation: {e}")
        except Exception as e: 
            logger.error(f"NovaSpark Error in micro_segment_and_enrich_lesson (Validation or other): {e}. Response: {response_str[:500]}", exc_info=True)
            # This will catch Pydantic's ValidationError as well
            raise ValueError(f"LLM response validation or other error in micro-segmentation: {e}")

nlp_llm_client = VertexAILLMClientForNLP(model_name=NLP_LLM_MODEL_NAME) 

app = FastAPI( 
    title="Uplas NLP Content Agent - NovaSpark Multilingual & Specificity Edition",
    description="Processes raw course content into structured, richly tagged, actionable, and strictly multilingual learning units using Vertex AI.",
    version="0.3.1" 
)

@app.post("/v1/process-course-content", response_model=ProcessedModule, status_code=status.HTTP_200_OK) 
async def process_content_endpoint(request_data: ProcessContentRequest, background_tasks: BackgroundTasks): 
    start_time = time.perf_counter()
    logger.info(f"NovaSpark NLP: Request for module_id: {request_data.module_id}, lang: {request_data.language_code}")

    if not GCP_PROJECT_ID or not nlp_llm_client.is_initialized: 
        logger.error("NovaSpark Critical: NLP service cannot operate (GCP_PROJECT_ID missing or LLM client not initialized).")
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="NLP service is not properly configured.")

    final_lessons_processed: List[NlpLesson] = []
    
    try:
        logger.info(f"NovaSpark NLP Stage 1: Starting macro-segmentation for module: {request_data.module_id}")
        macro_segments = await nlp_llm_client.macro_segment_module( 
            request_data.raw_text_content,
            request_data.language_code,
            request_data.module_title
        )
        logger.info(f"NovaSpark NLP Stage 1: Macro-segmentation complete. Found {len(macro_segments)} potential lessons.")

        for i, segment_info in enumerate(macro_segments): 
            lesson_title_from_macro = segment_info.get("lesson_title", f"Lesson {i+1} (Title N/A from macro)")
            start_idx = segment_info.get("text_segment_start_index")
            end_idx = segment_info.get("text_segment_end_index")

            # Rigorous index validation
            if not (isinstance(start_idx, int) and isinstance(end_idx, int) and \
                    0 <= start_idx <= end_idx <= len(request_data.raw_text_content)):
                logger.warning(f"NovaSpark NLP Warning: Skipping invalid segment (indices: {start_idx}-{end_idx}, text_len: {len(request_data.raw_text_content)}): {segment_info} for module {request_data.module_id}")
                continue
            
            # Skip if start_idx == end_idx resulting in empty snippet (unless it's a zero-length marker if that's ever valid)
            if start_idx == end_idx and start_idx < len(request_data.raw_text_content) : # Allow for marker at end if needed
                logger.info(f"NovaSpark NLP Info: Skipping zero-length segment for lesson: {lesson_title_from_macro}")
                continue


            lesson_text_snippet = request_data.raw_text_content[start_idx:end_idx]
            if not lesson_text_snippet.strip() and start_idx < end_idx : # Only skip if truly empty and not just whitespace
                logger.info(f"NovaSpark NLP Info: Skipping effectively empty lesson text snippet for lesson: {lesson_title_from_macro}")
                continue

            logger.info(f"NovaSpark NLP Stage 2: Starting micro-segmentation for lesson: '{lesson_title_from_macro}' (snippet length: {len(lesson_text_snippet)})")
            enriched_lesson_data_dict = await nlp_llm_client.micro_segment_and_enrich_lesson( 
                lesson_text_snippet,
                lesson_title_from_macro,
                request_data.language_code
            ) 
            
            current_lesson_topics_processed: List[NlpTopic] = []
            for topic_data_dict in enriched_lesson_data_dict.get("topics", []): 
                try:
                    processed_topic = NlpTopic(**topic_data_dict) 
                    current_lesson_topics_processed.append(processed_topic)
                except Exception as e_topic_val: 
                    logger.warning(f"NovaSpark NLP Warning: Failed to validate/process a topic from LLM output: {topic_data_dict.get('topic_id', 'N/A')}. Error: {e_topic_val}", exc_info=True)
            
            # Use title from enriched data if LLM provided it, otherwise fallback to macro-segmented title
            final_lesson_title = enriched_lesson_data_dict.get("lesson_title", lesson_title_from_macro) 
            lesson_summary_from_llm = enriched_lesson_data_dict.get("lesson_summary")

            if current_lesson_topics_processed: 
                final_lessons_processed.append(NlpLesson( 
                    lesson_title=final_lesson_title,
                    lesson_summary=lesson_summary_from_llm,
                    topics=current_lesson_topics_processed
                ))
                logger.info(f"NovaSpark NLP Stage 2: Lesson '{final_lesson_title}' processed with {len(current_lesson_topics_processed)} topics.")
            else:
                logger.warning(f"NovaSpark NLP Stage 2: No topics successfully processed for lesson '{final_lesson_title}'. Skipping this lesson.")

    except ValueError as ve: 
        logger.error(f"NovaSpark NLP ValueError during content processing for {request_data.module_id}: {ve}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Error processing content due to LLM response issue or validation: {str(ve)}")
    except Exception as e: 
        logger.error(f"NovaSpark NLP Unexpected error during content processing for {request_data.module_id}: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"An unexpected error occurred during content processing: {str(e)}")

    if not final_lessons_processed: 
        logger.warning(f"NovaSpark NLP Warning: No lessons were successfully processed for module: {request_data.module_id}. This might indicate issues with content or LLM responses.")
    
    end_time = time.perf_counter()
    processing_time_ms = (end_time - start_time) * 1000 

    logger.info(f"NovaSpark NLP: Successfully processed module_id: {request_data.module_id}. Time taken: {processing_time_ms:.2f} ms. Generated {len(final_lessons_processed)} lessons.")
    return ProcessedModule( 
        source_module_id=request_data.module_id,
        module_title=request_data.module_title,
        language_code=request_data.language_code,
        lessons=final_lessons_processed,
        processing_time_ms=round(processing_time_ms, 2),
        llm_model_used=nlp_llm_client.model_name
    )

@app.get("/health", status_code=status.HTTP_200_OK) 
async def health_check():
    service_name = "NLP_Content_Agent_NovaSpark_Multilingual_Specificity"
    if not GCP_PROJECT_ID or not NLP_LLM_MODEL_NAME: 
        return {"status": "unhealthy", "reason": "Required configurations (GCP_PROJECT_ID, NLP_LLM_MODEL_NAME) are missing.", "service": service_name, "innovate_ai_enhancements_active": True}
    if nlp_llm_client is None or not nlp_llm_client.is_initialized: 
         return {"status": "unhealthy", "reason": "Vertex AI client for NLP not properly initialized.", "service": service_name, "innovate_ai_enhancements_active": True}
    return {"status": "healthy", "service": service_name, "innovate_ai_enhancements_active": True}

if __name__ == "__main__": 
    import uvicorn
    logger.info(f"NovaSpark: Starting {app.title} v{app.version} for local development...")
    if not GCP_PROJECT_ID: 
        print("NovaSpark Warning: GCP_PROJECT_ID is not set. Please set this environment variable for the NLP agent.")
    
    port = int(os.getenv("PORT", 8005)) 
    uvicorn.run(app, host="0.0.0.0", port=port)
